{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711ebfbb3eb044bdb86aa2e9ce3bf6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c19d1541ee34120a88e4745124de054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7c970182724d009f263e5951e45dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be77f0eb52264969bbef26f1e4e40dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading aime_outputs/test2025-II/o4-mini/AIME_o4_mini_8_traj_25_II.jsonl with 15 samples\n",
      "Dataset loaded with 120 samples.\n",
      "Separator token is: \n",
      " {'input_ids': [198], 'attention_mask': [1]}\n",
      "dataset size after step filter: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 10199.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed dataset from 164_qwen_math_dataset.pkl\n",
      "Sanity check mode: max steps idx = [9, 981], max len idx = [1, 1637]\n",
      "Total number of samples: 164, 120\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [4] cannot be broadcast to indexing result of shape [20]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprm_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m----> 9\u001b[0m dataloader_benchmark \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_vanilla_inference_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LLM reasoning/code/DreamPRM/prm_data.py:506\u001b[0m, in \u001b[0;36mbuild_vanilla_inference_dataloader\u001b[0;34m(tokenizer, meta_batch_size)\u001b[0m\n\u001b[1;32m    503\u001b[0m         dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mmeta_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_merge_minibatch)\n\u001b[1;32m    505\u001b[0m         dataloader_benchmark[ds][model_out] \u001b[38;5;241m=\u001b[39m dataloader\n\u001b[0;32m--> 506\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataloader_benchmark\n",
      "File \u001b[0;32m~/miniconda3/envs/prm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/prm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/prm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/prm/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/LLM reasoning/code/DreamPRM/prm_data.py:190\u001b[0m, in \u001b[0;36mQwenMathDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens:\n\u001b[1;32m    189\u001b[0m     labels[(model_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSEP)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m--> 190\u001b[0m     \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEP\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(raw_labels)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     labels \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [4] cannot be broadcast to indexing result of shape [20]"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# from trl import PRMConfig, PRMTrainer\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "model = \"Qwen/Qwen2.5-Math-7B-Instruct\"\n",
    "from prm_data import *\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "tokenizer.add_special_tokens\n",
    "\n",
    "dataloader_benchmark = build_vanilla_inference_dataloader(\n",
    "    tokenizer=tokenizer,\n",
    "    meta_batch_size=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frozenwolf/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/frozenwolf/miniconda3/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at Qwen/Qwen2-0.5B and are newly initialized: ['score.bias', 'score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>', '<|im_end|>']\n",
      "Dataset loaded with 326767 samples.\n",
      "Filtering dataset to steps <= 20\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 322213\n",
      "Loading preprocessed dataset from pre_322213_qwen_math_dataset.pkl\n",
      "Filtering dataset to token size <= 2000\n",
      "Filtered dataset size: 322213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 322213/322213 [00:19<00:00, 16417.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed dataset from 1853280_qwen_math_dataset.pkl\n",
      "Sanity check mode: max steps idx = [20, 368], max len idx = [20, 1205]\n",
      "Total number of samples: 1853280, 322213\n",
      "\n",
      "\n",
      "Dataset loaded with 933 samples.\n",
      "Filtering dataset to steps <= 20\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 826/826 [00:02<00:00, 317.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering dataset to token size <= 2000\n",
      "Filtered dataset size: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 510/510 [00:00<00:00, 6900.55it/s]\n",
      "100%|██████████| 6429/6429 [00:01<00:00, 5038.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check mode: max steps idx = [20, 812], max len idx = [15, 1998]\n",
      "Total number of samples: 6429, 510\n",
      "\n",
      "\n",
      "Loading aime_outputs/test2025-II/o4-mini/AIME_o4_mini_8_traj_25_II.jsonl with 15 samples\n",
      "Dataset loaded with 120 samples.\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 11002.90it/s]\n",
      "100%|██████████| 164/164 [00:00<00:00, 1364.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check mode: max steps idx = [9, 981], max len idx = [1, 1637]\n",
      "Total number of samples: 164, 120\n",
      "\n",
      "\n",
      "Loading aime_outputs/test2025-II/o4-mini-8-diverse/AIME_o4_mini_diverse_traj_25_II.jsonl with 15 samples\n",
      "Dataset loaded with 120 samples.\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 10345.66it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 1208.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check mode: max steps idx = [2, 345], max len idx = [1, 1567]\n",
      "Total number of samples: 126, 120\n",
      "\n",
      "\n",
      "Loading aime_outputs/test2025-II/gemini/Gemini_AIME_Dataset_2024_25_II.jsonl with 15 samples\n",
      "Dataset loaded with 240 samples.\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 7581.61it/s]\n",
      "100%|██████████| 5171/5171 [00:00<00:00, 6672.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check mode: max steps idx = [46, 5714], max len idx = [35, 7921]\n",
      "Total number of samples: 5171, 240\n",
      "\n",
      "\n",
      "Loading aime_outputs/test2025-I/o4-mini/AIME_o4_mini_8_traj_25_I.jsonl with 15 samples\n",
      "Dataset loaded with 120 samples.\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 11212.22it/s]\n",
      "100%|██████████| 713/713 [00:00<00:00, 4424.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check mode: max steps idx = [29, 833], max len idx = [1, 1957]\n",
      "Total number of samples: 713, 120\n",
      "\n",
      "\n",
      "Loading aime_outputs/test2025-I/o4-mini-8-diverse/AIME_o4_mini_diverse_traj_25_I.jsonl with 15 samples\n",
      "Dataset loaded with 120 samples.\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:00<00:00, 11583.54it/s]\n",
      "100%|██████████| 122/122 [00:00<00:00, 1287.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check mode: max steps idx = [2, 377], max len idx = [1, 1580]\n",
      "Total number of samples: 122, 120\n",
      "\n",
      "\n",
      "Loading aime_outputs/test2025-I/gemini/Gemini_AIME_Dataset_2024_25_I.jsonl with 15 samples\n",
      "Dataset loaded with 240 samples.\n",
      "Separator token is: <|im_end|> {'input_ids': [151645], 'attention_mask': [1]}\n",
      "dataset size after step filter: 240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [00:00<00:00, 7216.52it/s]\n",
      "100%|██████████| 5538/5538 [00:00<00:00, 6545.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check mode: max steps idx = [48, 8627], max len idx = [44, 9377]\n",
      "Total number of samples: 5538, 240\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from prm_data import *\n",
    "# train_prm.py\n",
    "from datasets import load_dataset\n",
    "from trl import PRMConfig, PRMTrainer\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Qwen/Qwen2-0.5B\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
    "\n",
    "\n",
    "(\n",
    "    train_dataloader,\n",
    "    meta_dataloader,\n",
    "    dataloader_benchmark\n",
    ") = build_dataloader(\n",
    "    tokenizer=tokenizer,\n",
    "    train_batch_size= 1,\n",
    "    meta_batch_size= 1,\n",
    "    token_based=True,\n",
    "    add_new_token=False,\n",
    "    meta_dataset=\"AIME\",\n",
    "    sanity_check=False,\n",
    "    filter_dataset_steps=20,\n",
    "    filter_dataset_token_size=2000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForTokenClassification(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (score): Linear(in_features=896, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = PRMConfig(\n",
    "    output_dir=\"Output-Vannila-Qwen2.5-Math-7B-Instruct\",\n",
    "    eval_steps=10000,    \n",
    "    logging_dir=\"./logs\",     \n",
    "    logging_strategy=\"steps\",     \n",
    "    logging_steps=100,            \n",
    "    report_to=\"wandb\",         \n",
    "    save_strategy=\"epoch\",      \n",
    "    save_total_limit=2,            \n",
    "    num_train_epochs=5,   per_gpu_train_batch_size=10,  per_gpu_eval_batch_size=1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test2025-II with aime_outputs/test2025-II/o4-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:18<00:36, 18.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1:  14/15 = 0.9333\n",
      "Consensus (majority voting): 13/15 = 0.8667\n",
      "Pass@1: 0.25/1 = 0.2500\n",
      "Evaluating test2025-II with aime_outputs/test2025-II/o4-mini-8-diverse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:35<00:17, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1:  13/15 = 0.8667\n",
      "Consensus (majority voting): 12/15 = 0.8000\n",
      "Pass@1: 0.5/1 = 0.5000\n",
      "Evaluating test2025-II with aime_outputs/test2025-II/gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:16<00:00, 65.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@21:  13/15 = 0.8667\n",
      "Consensus (majority voting): 0/15 = 0.0000\n",
      "Pass@1: 0.6138930465246258/15 = 0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test2025-I with aime_outputs/test2025-I/o4-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 33%|███▎      | 1/3 [00:46<01:32, 46.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1:  12/15 = 0.8000\n",
      "Consensus (majority voting): 1/15 = 0.0667\n",
      "Pass@1: 1.7773809523809527/11 = 0.1616\n",
      "Evaluating test2025-I with aime_outputs/test2025-I/o4-mini-8-diverse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:18<00:38, 38.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@1:  11/15 = 0.7333\n",
      "Consensus (majority voting): 11/15 = 0.7333\n",
      "Pass@1: 11/15 = 0.7333\n",
      "Evaluating test2025-I with aime_outputs/test2025-I/gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:47<00:00, 115.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass@27:  13/15 = 0.8667\n",
      "Consensus (majority voting): 0/15 = 0.0000\n",
      "Pass@1: 0.7228954925936353/15 = 0.0482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from eval_aime import eval\n",
    "device=\"cuda\"\n",
    "with torch.no_grad():\n",
    "    for ds_name in dataloader_benchmark:\n",
    "        for model_name in tqdm(dataloader_benchmark[ds_name]):\n",
    "            print(f\"Evaluating {ds_name} with {model_name}\")\n",
    "            test_dataloader = dataloader_benchmark[ds_name][model_name]\n",
    "            predictions = None\n",
    "            for batch in test_dataloader:\n",
    "                \n",
    "                score = model(batch['input_ids'].to(device),\n",
    "                        batch['attention_mask'].to(device)).logits\n",
    "                # score -> (B, T,)\n",
    "                outputs = torch.argmax(score[:,-1], dim=-1).cpu()\n",
    "\n",
    "                outputs = outputs.to(dtype=torch.float32)\n",
    "                if predictions is None:\n",
    "                    predictions = outputs.numpy()\n",
    "                else:\n",
    "                    predictions = np.concatenate((predictions, outputs.numpy()), axis=0)\n",
    "            dataset = test_dataloader.dataset.dataset\n",
    "            predictions, score = eval(dataset, predictions, ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'problem', 'completions', 'generated_answers', 'gold_answer', 'is_correct', 'answers_correctness', 'majority_voting', 'majority_voting_is_correct', 'prompt', 'index', 'labels', 'subject'],\n",
       "     num_rows: 120\n",
       " }),\n",
       " array([0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.], dtype=float32),\n",
       " 'test2025-II')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, predictions, ds_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 847, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training_args = PRMConfig(output_dir=\"Qwen2.5-Math-7B-Instruct\")\n",
    "# trainer = PRMTrainer(model=model, args=training_args, processing_class=tokenizer, train_dataset=train_dataset)\n",
    "# trainer.train()\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# pipe = pipeline(\"token-classification\", model=\"trl-lib/Qwen2-0.5B-Reward-Math-Sheperd\")\n",
    "# dataset = load_dataset(\"trl-lib/math_shepherd\")\n",
    "# example = {\n",
    "#     \"prompt\": \"Musa is the class teacher of a class of 45 students. He wants to split them into three groups by age. If a third of the class is under 11 years, and two-fifths are above 11 but under 13, how many students will be in the third group (13 years and above)?\",\n",
    "#     \"completions\": [\n",
    "#         \"Step 1: A third of the class is under 11 years because 11 - 1/3 = <<11-1/3=7>>7.\",\n",
    "#         \"Step 2: Two-fifths of the class are above 11 but under 13 because 2/5 * 11 = <<2/5*11=8>>8.\",\n",
    "#         \"Step 3: There are 45 students, so the third group will have 45 - 7 - 8 = <<45-7-8=20>>20 students. The answer is: 20\",\n",
    "#     ],\n",
    "#     \"labels\": [True, False, False],\n",
    "# }\n",
    "\n",
    "\n",
    "separator = \"\\n\"  # It's important to use the same separator as the one used during training\n",
    "\n",
    "for idx in range(1, len(example[\"completions\"]) + 1):\n",
    "    steps = example[\"completions\"][0:idx]\n",
    "    text = separator.join((example[\"prompt\"], *steps)) + separator  # Add a separator between the prompt and each steps\n",
    "    pred_entity = pipe(text)[-1][\"entity\"]\n",
    "    pred = {\"LABEL_0\": False, \"LABEL_1\": True}[pred_entity]\n",
    "    label = example[\"labels\"][idx - 1]\n",
    "    print(f\"Step {idx}\\tPredicted: {pred} \\tLabel: {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
